### nn-init-demo

Shows the "vanishing gradient" problem and "dying ReLU" problems, as well as the problems of the activations going to zero if we do the usual "small, random weight" initialization technique. Then we investigate the Xavier init, He init, and Batch normalization techniques for combatting this issue. 
